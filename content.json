{"meta":{"title":"Hexo","subtitle":"","description":"","author":"John Doe","url":"http://example.com","root":"/"},"pages":[],"posts":[{"title":"june","slug":"june","date":"2021-06-05T12:22:29.000Z","updated":"2021-06-05T12:22:29.038Z","comments":true,"path":"2021/06/05/june/","link":"","permalink":"http://example.com/2021/06/05/june/","excerpt":"","text":"","categories":[],"tags":[]},{"title":"","slug":"HRNet论文代码解读","date":"2021-06-05T12:03:14.132Z","updated":"2021-03-04T12:49:25.426Z","comments":true,"path":"2021/06/05/HRNet论文代码解读/","link":"","permalink":"http://example.com/2021/06/05/HRNet%E8%AE%BA%E6%96%87%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/","excerpt":"","text":"HRNet论文代码解读 前情提要好记性不如烂笔头！ 介绍 以前相关工作 对于人体姿态估计这种 position-sensitive 位置敏感任务，以往的方法大多参考经典的分类网络：先降低分辨率提取丰富语义低分辨率表征，再恢复高分辨率，找到对应的位置信息。 这种恢复分辨率的方法其实无法真正获取足够有效的高分辨率位置信息。 新的方法（总览） HRNet提出一种新的特征提取方法，创新之处在于要从头到尾 维持 一条高分辨率表征分支。 HRNet 整体采用 并行 结构，共分为4个阶段。 第1阶段是1个高分辨率的子网络，此后逐步添加 由高到低 的分辨率子网络来组成新的阶段；第2阶段2种分辨率，第3阶段3种分辨率，以此类推。然后在不同阶段的多种分辨率之间，采用 多尺度融合 来交换信息。最后在网络的高分辨率输出上预测 keypoints 关键点。 方法（多人姿态估计） HRNet 最初是在 top-down 流派下设计的，为了提升关键点检测准确率。 但是 HRNet 作为一种特征提取网络，其实是没有流派区别的，既可以用在 top-down ，也可以用在 bottom-up 上。 （上图为 top-down 流程，下图为 bottom-up 流程） 这里以 bottom-up 方法做示例。HRNet主要应用在 关键点检测模块 。 问题定义因为 HRNet 提出的方法主要应用在 关键点检测 上，所以下面的分析不放大到姿态估计的全貌，而是聚焦在关键点检测部分的特征提取部分。 为了方便我整个系列的进行，以后我们就叫他 backbone 骨干网络 。 backbone骨干网络的输入输出是这种形式： 输入：feature maps 特征图（文中为1/4分辨率） 输出：feature maps 特征图（跟输入一致） 框架分析 High-Resolution Network之前我们总览了整个网络，在这部分主要介绍论文里提出的重点方法，下一部分再通过实例化+代码解析的形式展现全貌。 网络组成 stem: 输入图片经过 由两个步幅为 2 的 $3*3$ 卷积组成的 Stem，将分辨率降低为原来的 $\\frac{1}{4}$ ； 输入 原图 输出 $\\frac{1}{4}$ 分辨率特征图（同ResNet） ==main body==: 包含 parallel multi-resolution convs., repeated multi-resolution fusions, representation head 等部分； 输入 $\\frac{1}{4}$ 分辨率特征图 输出 $\\frac{1}{4}$ 分辨率特征图 接下来介绍在 main body 当中的重点部分！ Parallel Multi-Resolution Convolutions.并行的多分辨率卷积 HRNet 一共有4个 stage 阶段。这4个阶段远看符合常规的 high-to-low 由高到低 特征提取结构；但是每个阶段拥有不同分辨率的 stream（代码里对应的branch），新的阶段保留原有分辨率并新增加前一阶段的 $\\frac{1}{2}$ 分辨率表征。（值得注意，为了保留信息，分辨率每降低一半，通道数翻倍。） 同时使用一种 parallel 并行 的结构将他们组合，类似这种： （竖着看；下标的第一个数字代表阶段，第二个数字代表分辨率） 简单地实例化一下网络，以 HRNet-w32 为例： stage resolution channels stage1 $\\frac{1}{4}$ 32 stage2 $\\frac{1}{4}$, $\\frac{1}{8}$ 64 stage3 $\\frac{1}{4}$, $\\frac{1}{8}$, $\\frac{1}{16}$ 128 stage4 $\\frac{1}{4}$, $\\frac{1}{8}$, $\\frac{1}{16}$, $\\frac{1}{32}$ 256 Repeated Multi-Resolution Fusions.重复的多分辨率融合 不同阶段之间，使用重复的多尺度融合来进行信息交换。 交换阶段如图中的画圈部分： 信息交换需要的操作，有两种： 下采样：使用步幅为2的 $33$ 卷积，分辨率下降2x；或者连续两个步幅为2的 $33$ 卷积，分辨率下降4x； 上采样：用 nearest neighbor 上采样，后接 $1*1$ 卷积来统一通道数； (如上图) 一般有两个情况，有多少个输入就有多少个输出（stage 4） or 输出比输入多一个$\\frac{1}{2}$ 最低分辨率输出（stage 1, 2, 3）； 每个输出都要是前面所有输入的聚合；根据当前状态使用上、下采样操作或直接连接； 以 3 种不同分辨率的 fusion 融合 为例： 输入：3种分辨率 ${R_r^i, r = 1, 2, 3}$ 输出：3种分辨率 ${R_r^o, r = 1, 2, 3}$ $f_{xr}(·)$ fusion 函数，看人下菜碟，根据过来的分辨率和要输出的分辨率选择上、下采样或原样。 输出是输入经过分辨率变换后的相加 $R_r^o = f_{1r}(R_1^i) + f_{2r}(R_2^i) + f_{3r}(R_3^i)$； 如果是跨阶段，比如stage3到stage4，就需要一个额外的输出，小 $\\frac{1}{2}$ 分辨率。 Representation Head表征头 涉及到输出阶段，取决于后续要应用到的任务，图示 分应用来说： 有三种，文中分别用来对应不同的任务。 HRNetV1: (a) 仅输出最高分辨率分支的表征。一般用于人体姿态估计。 HRNetV2: (b) 直接通过双线性插值的上采样形式统一分辨率到最高，相连，后接 $1*1$ 卷积来混合表征。一般用于语义分割。 HRNetV2p: (c) 将V2当中的输出，通过下采样操作分别采样到多个分辨率。一般用于目标检测。 实例化网络文章参考 ResNet 的设计思路来分配每阶段的网络深度以及每种分辨率表征的通道数。 以 HRNet-W32 为例： 4个 stage 阶段（括号里是分辨率）； stage1：（$\\frac{1}{4}$）4 residual units；每个unit 包含 1 bottleneck block（channels 64）；后接 $3*3$ 卷积将通道数减为 C=32 ; stage2：（$\\frac{1}{8}$）1 modularized block； stage3：（$\\frac{1}{16}$）4 modularized blocks； stage4：（$\\frac{1}{32}$）3 modularized blocks； 每个 modularized block 包含两部分：multi-resolution parallel convolutions 和 multi-resolution fusions; 在 multi-resolution parallel convolutions 里的每个 branch 都有 4 个 residual units； 每个 units 包含 2 个 $3*3$ 卷积，后接 BN 和 ReLU； 4个 parallel subnetworks 并行子网络； 子网络的 resolution 分辨率 减半的同时 channels 通道数 加倍； 单独梳理一下名词之间的关系： 有4个 stage； 每个 stage 由 modularized blocks 组成；modularized blocks 数量分别是 1，1，4，3 ； 每个 modularized blocks 在 stage 1，2，3，4 分别有 1，2，3，4 个 branches; 不同 branch 代表不同的分辨率； 每个 branch 由 4 个 residual units 和 1 个 multi-resolution fusion unit 组成； 用一个表格来表示： 表格当中，[·]是residual units，有 basic 版本和 bottleneck 版本；第二个数字表示 residual units 重复的次数；第三个数字表示 整个 modularized blocks 重复的次数；C 是 channels 通道数的意思。 按照 ResNet 的 PyTorch 实现流程，将其中 $77$ 卷积替换为2 个 $33$ 卷积；然后将分辨率降低为 $\\frac{1}{4}$ 。 代码mmpose configs backbones keypoints_head configs定位 mmpose/configs/bottom_up/hrnet/coco/hrnet_w32_coco_512x512.py 整个configs文件可以分为以下几个大块： #runtime setting #optimizer #model setting runtime setting一些常规的设置，比如log，和加载checkpoints，工作流等等。 evaluation 用来设定训练阶段的评价指标和检验的间隔。 12345678910111213# runtime settinglog_level = &#x27;INFO&#x27;load_from = None # 给定路径，加载预训练模型；从头开始，不恢复训练。resume_from = None # 从上次保存好的checkpoints恢复训练。dist_params = dict(backend=&#x27;nccl&#x27;) # Parameters to setup distributed training, the port can also be setworkflow = [(&#x27;train&#x27;, 1)] # Workflow for runner. [(&#x27;train&#x27;, 1)] means there is only one workflow and the workflow named &#x27;train&#x27; is executed once# Config to set the checkpoint hook, Refer to https://github.com/open-mmlab/mmcv/blob/master/mmcv/runner/hooks/checkpoint.py for implementationcheckpoint_config = dict(interval=50) # 每隔多少interval保存一次checkpoints。# Config of evaluation during trainingevaluation = dict(interval=50, metric=&#x27;mAP&#x27;, key_indicator=&#x27;AP&#x27;)# 训练期间：使用evaluation的间隔；哪种评价指标；以什么为关键指标来保存最佳checkpoint optimizer一些关于优化器的配置。 1234567891011121314optimizer = dict( type=&#x27;Adam&#x27;, lr=0.0015,)optimizer_config = dict(grad_clip=None) # Do not use gradient clip# learning policylr_config = dict( policy=&#x27;step&#x27;, # Policy of scheduler warmup=&#x27;linear&#x27;, # Type of warmup used. warmup_iters=500, # The number of iterations or epochs that warmup warmup_ratio=0.001, # LR used at the beginning of warmup equals to warmup_ratio * initial_lr step=[200, 260]) # Steps to decay the learning ratetotal_epochs = 300 # Total epochs to train the model 存放配置文件，这里主要看 # model setting 部分。 12345678910111213141516171819202122232425262728293031323334353637# model settingsmodel = dict( type=&#x27;BottomUp&#x27;, pretrained=&#x27;https://download.openmmlab.com/mmpose/&#x27; &#x27;pretrain_models/hrnet_w32-36af842e.pth&#x27;, # backbone 来源 `mmpose/mmpose/models/backbones/hrnet.py` backbone=dict( type=&#x27;HRNet&#x27;, in_channels=3, extra=dict( stage1=dict( num_modules=1, num_branches=1, block=&#x27;BOTTLENECK&#x27;, num_blocks=(4, ), num_channels=(64, )), stage2=dict( num_modules=1, num_branches=2, block=&#x27;BASIC&#x27;, num_blocks=(4, 4), num_channels=(32, 64)), stage3=dict( num_modules=4, num_branches=3, block=&#x27;BASIC&#x27;, num_blocks=(4, 4, 4), num_channels=(32, 64, 128)), stage4=dict( num_modules=3, num_branches=4, block=&#x27;BASIC&#x27;, num_blocks=(4, 4, 4, 4), num_channels=(32, 64, 128, 256))), ), 对应实例化！(hrnet_w32) backbones 定位 mmpose/configs/bottom_up/hrnet/coco/hrnet_w32_coco_512x512.py ResNet 块 （左图是BasicBlock，右图是Bottleneck） HRModule 分支模块 判断输入输出； 执行（循环）分支创建，每条分支内部分辨率相同； 以图为例，stage2 的分支的创建需要经过两步： 第一步，在 stage2 经过 _check_barnches 多出一条低分辨率分支，通过下一趴会提到的 _make_transition_layer 创建一个过渡的低分辨率分支； 第二步，_make_one_branch顺序搭建指定数目的 blocks；然后用 _make_branches 循环搭建；注意这里只有第一个block需要考虑升降维情况。 配置代码如下： 融合模块 判断分支数，如果是1条分支，不用融合； 根据论文里的图示形式来融合分支； 以 stage2 为例，融合模块在这里： 融合形式如下： HRNet 创建过渡层 _make_transition_layer 判断新的 stage 是否需要生成多一个 生成下一个 stage 的输入特征 创建层和阶段 _make_layer _make_stage 利用HRModule里写的函数来做 结果（HPE） 数据集上的结果 mmpose版本，精度提升 （top-down） （bottom-up） 内存和速度 消融实验 ablation study Reference源码 https://mmpose.readthedocs.io/en/latest/tutorials/0_config.html https://github.com/open-mmlab/mmpose 博客 https://zhuanlan.zhihu.com/p/152594622 https://blog.csdn.net/weixin_38715903/article/details/101629781 https://www.cnblogs.com/pprp/p/12750692.html 论文 https://arxiv.org/abs/1908.07919 Sun K, Xiao B, Liu D, et al. Deep high-resolution representation learning for human pose estimation[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2019: 5693-5703. Wang J, Sun K, Cheng T, et al. Deep high-resolution representation learning for visual recognition[J]. IEEE transactions on pattern analysis and machine intelligence, 2020. Cheng B, Xiao B, Wang J, et al. Higherhrnet: Scale-aware representation learning for bottom-up human pose estimation[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020: 5386-5395.","categories":[],"tags":[]},{"title":"hi","slug":"hi","date":"2021-06-05T11:48:21.000Z","updated":"2021-06-05T11:48:21.738Z","comments":true,"path":"2021/06/05/hi/","link":"","permalink":"http://example.com/2021/06/05/hi/","excerpt":"","text":"","categories":[],"tags":[]},{"title":"Hello World","slug":"hello-world","date":"2021-06-05T11:39:06.497Z","updated":"2021-06-05T11:39:06.497Z","comments":true,"path":"2021/06/05/hello-world/","link":"","permalink":"http://example.com/2021/06/05/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[],"tags":[]}],"categories":[],"tags":[]}